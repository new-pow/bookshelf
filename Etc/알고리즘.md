# 알고리즘
---
- 알고리즘 과목이 다루는 주된 내용?
	- 다양한 문제 풀이 방법이 존재하다
	- 상황에 따라 답이 다름
- 정렬 알고리즘
	- 버블, 선택, 삽입, 셸, 합병, 퀵, 힙, 계수, 기수 정렬
- 탐색 알고리즘
	- 순차, 이진, 탐색트리, 해싱
- 최소 신장트리
	- 욕심쟁이, 프림, 크루스칼, 다익스트라
- 문제 푸는 방법과 그 방법의 효율을 분석하는 것이 주된 내용
	- 동일 입력 동일 출력일 때
	- 공간(메모리) 사용은 최소한으로
	- 수행 시간은 빠르게
- 컴퓨터 알고리즘 정의
	- 주어진 문제를 풀기 위한 명령어들의 단계적 나열
	- 입출력
	- 명확성: 단순 명료해야함
	- 유한성: 반드시 종료되어야함
	- 유효성: 모든 명령어는 수행 가능해야함
---
- 최댓값 찾기
	- 방법1. 값을 하나씩 모두 비교해가며 찾기
	- 방법2. 인접한 2개씩 찾기. 토너먼트 방식
	- 방법3. 무작위로 찾기. 최악의 경우 100번 수행
	- 방법4. 선형 비교. 1씩 증가시켜서 찾기. 
	- 방법5. 중앙값을 사용하여 범위를 반씩 줄이면서 찾기
---
## 알고리즘 설계 기법
- 주어진 문제, 속성, 조건 등ㅣ 매우 다양
	- 범용적인 설계 기법은 미존재
- 대적인 알고리즘 설계 기법
	- 분할정책 divide and comquer
	- 동적 프로그래밍 dynamic programming (약 90%)
	- 욕심쟁이 방법 greedy (잘 안씀;)
---
## 알고리즘 성능 분석
- 공간 복잡도
	- 실행에 필요한 저장 공간
	- 동일 입력/출력 일 때 저장공간을 적게 사용하면 좋은 알고리즘
- 시간 복잡도
	- 실행에 걸리는 시간
	- 동일 입력/출력일 때 빠르면 좋은 알고리즘
- 공간 복잡도보다 시간복잡도가 중요하다
	- 공간은 재사용 가능, 확장 가능함
- 빅오 표기법 Big-O
### 시간 복잡도
- 입력 데이터 크기 n이 증가하면 수행 시간도 증가
- 입력크기 n에 대한 함수 1(n)으로 표현
- 입력 데이터의 상태에 종속적
	- 최선 수행시간
	- 최악 수행시간
	- 평균 수행시간
- 점근 성능
	- 순환문이 있으면 n
	- 중첩 순환문이 있으면 n^2
	- 수행시간의 다항식 함수에서 최고 차항만을 계수 없이 취해서 표현
	- 수행시간의 정확한 값이 아닌 어림값
	- 수행 시간의 증가 추세를 파악하는데 용이
	- 알고리즘의 우열 표현이 용이
- 점근 성능 증가 추세
	- **O(1)** - 상수 시간 복잡도: 데이터 크기에 상관없이 항상 일정한 시간이 소요됨. 예: 배열의 인덱스 접근, 해시 테이블 조회(최선의 경우)
	- **O(log n)** - 로그 시간 복잡도: 데이터 크기가 커져도 실행 시간이 천천히 증가함. 예: 이진 탐색, 균형 이진 트리 연산
	- **O(n)** - 선형 시간 복잡도: 데이터 크기에 비례하여 실행 시간이 증가함. 예: 배열 순회, 선형 탐색
	- **O(n log n)** - 선형 로그 시간 복잡도: 대부분의 효율적인 정렬 알고리즘의 시간 복잡도. 예: 퀵 정렬(평균), 머지 정렬, 힙 정렬
	- **O(n²)** - 이차 시간 복잡도: 데이터 크기의 제곱에 비례하여 실행 시간이 증가함. 예: 버블 정렬, 삽입 정렬, 선택 정렬
	- **O(n³)** - 삼차 시간 복잡도: 데이터 크기의 세제곱에 비례하여 실행 시간이 증가함. 예: 일부 행렬 연산, 플로이드-워셜 알고리즘
	- **O(2ⁿ)** - 지수 시간 복잡도: 데이터 크기가 증가함에 따라 실행 시간이 기하급수적으로 증가함. 예: 재귀 피보나치, 부분집합 생성
	- **O(n!)** - 팩토리얼 시간 복잡도: 가장 비효율적인 시간 복잡도 중 하나. 예: 외판원 문제(브루트 포스), 순열 생성
- 기본 점화식과 폐쇄형
## 분할 정복 방법
- 순환적으로 문제를 푸는 하향식 접근 방법
- 처리 단계
	- 분할
		- 작은 문제로 순환적으로 분할한다
	- 정복
		- 최소 단위가 되면 작은 문제의 해를 구한다
	- 결합
		- 작은 문제에 대해 정복된 해를 결합하여 원래 문제의 해를 구한다
		- 결합 단계가 없는 문제도 존재한다
## 퀵 정렬(Quick Sort)

퀵 정렬은 효율적이고 널리 사용되는 정렬 알고리즘으로, 분할 정복(divide-and-conquer) 전략을 기반으로 합니다.

### 기본 원리

1. **피벗 선택**: 배열에서 하나의 요소를 피벗(pivot)으로 선택합니다.
2. **분할(Partitioning)**: 피벗보다 작은 요소는 피벗의 왼쪽으로, 큰 요소는 오른쪽으로 이동시킵니다.
3. **재귀적 정렬**: 피벗을 기준으로 나뉜 두 부분 배열에 대해 재귀적으로 퀵 정렬을 적용합니다.

### 알고리즘 단계

1. 피벗을 선택합니다(일반적으로 첫 번째, 마지막, 또는 중간 요소).
2. 피벗을 기준으로 배열을 재배치합니다.
3. 피벗의 최종 위치를 확정합니다.
4. 피벗의 좌측과 우측 부분 배열에 대해 재귀적으로 정렬합니다.
5. 부분 배열의 크기가 1 이하가 되면 재귀를 종료합니다.

### 시간 복잡도
- **최선의 경우**: O(n log n) - 피벗이 항상 배열을 균등하게 분할할 때
- **평균적인 경우**: O(n log n)
- **최악의 경우**: O(n²) - 이미 정렬된 배열에서 첫 번째 또는 마지막 요소를 피벗으로 선택할 때 (역순)

### 공간 복잡도
- O(log n) - 재귀 호출 스택 공간

### 특징
- **제자리 정렬(In-place sorting)**: 추가 메모리 공간을 거의 필요로 하지 않습니다.
- **불안정 정렬(Unstable sorting)**: 동일한 값을 가진 요소들의 상대적 순서가 바뀔 수 있습니다.
- **적응형(Adaptive)**: 입력 데이터에 따라 성능이 달라집니다.
- **분할 정복**: 문제를 작은 부분 문제로 나누어 해결합니다.

### 최적화 방법
- **랜덤 피벗 선택**: 최악의 경우를 피하기 위해 무작위로 피벗을 선택
- **세 값의 중앙값(Median-of-three)**: 첫 번째, 중간, 마지막 요소 중 중앙값을 피벗으로 선택
- **작은 부분 배열에 삽입 정렬 사용**: 작은 크기의 부분 배열에 대해서는 삽입 정렬이 더 효율적일 수 있음
- **꼬리 재귀 제거(Tail recursion elimination)**: 스택 오버플로우를 방지하기 위해 꼬리 재귀 최적화

퀵 정렬은 평균적으로 O(n log n)의 빠른 성능을 보이므로 실제 응용에서 널리 사용되며, 많은 프로그래밍 언어의 표준 라이브러리에서 정렬 알고리즘으로 채택되고 있습니다.

---
## 동적 프로그래밍(Dynamic Programming)
- 문제의 크기가 작은 소문제에 대한 해를 저장해놓고 이를 이용하여 크기보다 큰 문제의 해를 점진적으로
- 분할과 동적의 차이점
	- 분할은 서로 독립적이다
	- 동적은 서로 연결이 되어있다. 답들이 모두 연계되어 있다.
동적 프로그래밍은 복잡한 문제를 간단한 하위 문제들로 나누어 해결하는 알고리즘 기법입니다. 이 방법은 특히 최적화 문제를 해결할 때 매우 효과적입니다.

### 핵심 원리
1. **부분 문제 중복(Overlapping Subproblems)**: 동일한 작은 문제들이 반복적으로 발생합니다.
2. **최적 부분 구조(Optimal Substructure)**: 큰 문제의 최적해가 작은 문제들의 최적해로부터 구성됩니다.

### 구현 방식
동적 프로그래밍은 두 가지 방식으로 구현할 수 있습니다:

#### 1. 하향식 접근법(Top-down, 메모이제이션)
- 재귀적으로 문제를 해결하되, 이미 계산된 결과를 저장하고 재활용합니다.
- 필요한 부분만 계산하는 장점이 있습니다.

```python
def fib_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 2:
        return 1
    memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
    return memo[n]
```

#### 2. 상향식 접근법(Bottom-up, 테이블 채우기)
- 가장 작은 부분 문제부터 시작하여 전체 문제로 확장합니다.
- 일반적으로 반복문을 사용하며 메모리 사용이 효율적입니다.

```python
def fib_tabulation(n):
    dp = [0] * (n+1)
    dp[1] = dp[2] = 1
    
    for i in range(3, n+1):
        dp[i] = dp[i-1] + dp[i-2]
    
    return dp[n]
```

### 대표적인 동적 프로그래밍 문제
1. **피보나치 수열**: 앞의 두 수의 합으로 현재 수를 계산합니다.
2. **최장 공통 부분 수열(LCS)**: 두 문자열에서 공통으로 나타나는 가장 긴 부분 수열을 찾습니다.
3. **배낭 문제(Knapsack Problem)**: 제한된 무게 내에서 최대 가치를 가지도록 물건을 선택합니다.
4. **최단 경로 문제**: 그래프에서 두 노드 간의 최단 경로를 찾습니다.

### 장단점

#### 장점
- 시간 복잡도를 크게 줄일 수 있습니다(예: 피보나치 - 지수 시간에서 선형 시간으로)
- 복잡한 문제를 체계적으로 해결할 수 있습니다.

#### 단점
- 메모리를 추가로 사용합니다(공간 복잡도 증가).
- 모든 문제에 적용할 수 있는 것은 아닙니다.

동적 프로그래밍은 효율적인 알고리즘 설계에 매우 중요한 패러다임으로, 컴퓨터 과학의 많은 분야에서 활용되고 있습니다.

---
## 분할정복과 동적 프로그래밍의 차이점

분할정복(Divide and Conquer)과 동적 프로그래밍(Dynamic Programming)은 모두 문제를 작은 부분으로 나누어 해결하는 알고리즘 패러다임이지만, 몇 가지 중요한 차이점이 있습니다.

### 주요 차이점

#### 1. 부분 문제의 중복성

- **분할정복**: 부분 문제들이 서로 독립적이며 중복되지 않습니다.
- **동적 프로그래밍**: 동일한 부분 문제가 여러 번 재사용되고 중복됩니다.

#### 2. 문제 해결 방식

- **분할정복**: 문제를 독립적인 하위 문제로 분할하고, 각각을 재귀적으로 해결한 후 결합합니다.
- **동적 프로그래밍**: 작은 부분 문제들의 해결책을 저장(메모이제이션)하고 재활용하여 전체 문제를 해결합니다.

#### 3. 계산 결과 저장

- **분할정복**: 부분 문제의 결과를 저장하지 않고 매번 새롭게 계산합니다.
- **동적 프로그래밍**: 이미 계산된 부분 문제의 결과를 테이블이나 배열에 저장하여 재활용합니다.

#### 4. 적용 조건

- **분할정복**: 부분 문제가 독립적일 때 적합합니다.
- **동적 프로그래밍**: 최적 부분 구조(optimal substructure)와 중복 부분 문제(overlapping subproblems)가 있을 때 효과적입니다.

### 예시 비교

#### 병합 정렬 (분할정복의 예)

- 배열을 반으로 나눠 각각 정렬한 후 병합합니다.
- 각 부분 배열은 독립적으로 처리되며, 부분 문제의 결과가 중복되지 않습니다.

#### 피보나치 수열 (동적 프로그래밍의 예)

- 순수 재귀 구현(분할정복): F(n) = F(n-1) + F(n-2)는 많은 중복 계산이 발생합니다.
- DP 구현: 이미 계산된 F(i)값을 저장하여 재사용함으로써 중복 계산을 방지합니다.

### 시간 복잡도 차이

- **분할정복**: 같은 계산을 반복적으로 수행할 수 있어 효율성이 떨어질 수 있습니다.
- **동적 프로그래밍**: 중복 계산을 방지하여 시간 복잡도를 크게 개선할 수 있습니다.

이러한 차이점 때문에, 문제의 특성에 따라 적절한 알고리즘 패러다임을 선택하는 것이 중요합니다. 부분 문제가 독립적이면 분할정복, 부분 문제가 중복되면 동적 프로그래밍이 더 효율적입니다.

---

## 욕심쟁이 방법
## 그리디 알고리즘(Greedy Algorithm)

그리디 알고리즘은 최적화 문제를 해결하기 위한 접근 방법으로, 각 단계에서 지금 당장 최적인 선택(locally optimal choice)을 하는 전략입니다. 이러한 지역적 최적 선택이 전체 문제에 대한 최적해로 이어질 것이라는 기대에 기반합니다.

### 핵심 원리

1. **탐욕적 선택 속성(Greedy Choice Property)**: 현재 상황에서 가장 좋아 보이는 선택을 합니다.
2. **지역적 최적해(Local Optimal Solution)**: 각 단계에서 최적의 결정을 내립니다.
3. **단순함**: 일반적으로 구현이 간단하고 이해하기 쉽습니다.

### 그리디 알고리즘의 작동 방식

1. 문제를 여러 단계로 나눕니다.
2. 각 단계에서 현재 상황에서 가장 좋은 선택을 합니다.
3. 한 번 결정한 것은 다시 고려하지 않습니다.

### 대표적인 그리디 알고리즘 문제

#### 1. 동전 교환 문제(Coin Change Problem)

- 가장 큰 단위의 동전부터 사용하여 특정 금액을 만드는 방법
- 예: 거스름돈 1260원을 500원, 100원, 50원, 10원 동전으로 만들기

```python
def coin_change_greedy(amount, coins):
    coins.sort(reverse=True)  # 내림차순 정렬
    count = 0
    for coin in coins:
        count += amount // coin  # 해당 동전을 최대한 사용
        amount %= coin  # 남은 금액 계산
    return count
```

#### 2. 활동 선택 문제(Activity Selection Problem)

- 한 자원을 사용하는 여러 활동 중에서 가능한 많은 활동을 선택하는 문제
- 종료 시간이 빠른 활동부터 선택하는 전략

#### 3. 크루스칼 알고리즘(Kruskal's Algorithm)

- 가중치가 작은 간선부터 선택하여 최소 신장 트리를 구성하는 알고리즘

#### 4. 허프만 코딩(Huffman Coding)

- 데이터 압축에 사용되는 알고리즘으로, 발생 빈도가 높은 문자에 짧은 코드를 할당

### 그리디 알고리즘의 장단점

#### 장점

- 일반적으로 구현이 간단하고 빠릅니다.
- 복잡한 문제를 직관적으로 해결할 수 있습니다.
- 계산 효율성이 높습니다.

#### 단점

- 항상 최적해를 보장하지는 않습니다.
- 전체적인 상황을 고려하지 않기 때문에 부분 최적해에 머물 수 있습니다.
- 특정 구조(그리디 선택 속성과 최적 부분 구조)를 가진 문제에만 적용 가능합니다.

### 그리디 vs 동적 프로그래밍

- **그리디**: 현재 상태에서 최선의 선택만 고려하고, 이전 선택을 다시 검토하지 않습니다.
- **동적 프로그래밍**: 모든 가능한 선택을 고려하고, 부분 문제의 해결책을 저장하여 재사용합니다.

그리디 알고리즘은 문제에 따라 항상 최적해를 찾지 못할 수 있으므로, 사용 전에 해당 문제가 그리디 접근법으로 최적해를 얻을 수 있는지 증명이 필요합니다.

---
## 최소 신장 트리(Minimum Spanning Tree, MST)

최소 신장 트리는 그래프 이론에서 중요한 개념으로, 가중치가 있는 무방향 연결 그래프에서 모든 정점을 포함하면서 간선의 가중치 합이 최소인 부분 그래프를 말합니다.

### 핵심 특징

1. **신장 트리(Spanning Tree)의 성질**:
    
    - 그래프의 모든 정점을 포함합니다.
    - 정확히 (정점 수 - 1)개의 간선을 가집니다.
    - 사이클이 없는 연결된 부분 그래프입니다.
2. **최소 가중치**:
    
    - 가능한 모든 신장 트리 중에서 간선의 가중치 합이 최소입니다.

### 대표적인 최소 신장 트리 알고리즘

#### 1. 크루스칼 알고리즘(Kruskal's Algorithm)

가중치가 작은 간선부터 차례로 선택하여 MST를 구성합니다.

**작동 방식**:

1. 모든 간선을 가중치 오름차순으로 정렬합니다.
2. 가중치가 가장 작은 간선부터 선택합니다.
3. 선택한 간선이 사이클을 형성하면 제외합니다.
4. (정점 수 - 1)개의 간선이 선택될 때까지 반복합니다.

**시간 복잡도**: O(E log E) (E는 간선의 수)

**구현 예시**:

```python
def kruskal(graph, vertices):
    result = []
    i, e = 0, 0
    # 간선을 가중치 기준으로 정렬
    graph = sorted(graph, key=lambda item: item[2])
    parent = [i for i in range(vertices)]
    rank = [0] * vertices
    
    # Union-Find 알고리즘 사용
    def find(i):
        if parent[i] != i:
            parent[i] = find(parent[i])
        return parent[i]
    
    def union(i, j):
        root_i = find(i)
        root_j = find(j)
        if root_i != root_j:
            if rank[root_i] < rank[root_j]:
                parent[root_i] = root_j
            else:
                parent[root_j] = root_i
                if rank[root_i] == rank[root_j]:
                    rank[root_i] += 1
    
    while e < vertices - 1:
        u, v, w = graph[i]
        i += 1
        x = find(u)
        y = find(v)
        if x != y:
            e += 1
            result.append([u, v, w])
            union(x, y)
    
    return result
```

#### 2. 프림 알고리즘(Prim's Algorithm)

시작 정점에서부터 트리를 확장해나가는 방식으로 MST를 구성합니다.

**작동 방식**:

1. 임의의 시작 정점을 선택합니다.
2. 현재 트리에 연결된 간선 중 가중치가 가장 작은 간선을 선택합니다.
3. 해당 간선이 연결하는 새로운 정점을 트리에 추가합니다.
4. 모든 정점이 포함될 때까지 반복합니다.

**시간 복잡도**: O(E log V) (우선순위 큐 사용 시)

### 최소 신장 트리의 응용

1. **네트워크 설계**:
    
    - 통신 네트워크, 전력망, 도로망 등의 최소 비용 설계에 활용됩니다.
2. **클러스터링**:
    
    - 데이터 마이닝과 기계학습에서 클러스터링 알고리즘으로 사용됩니다.
3. **근사 알고리즘**:
    
    - 외판원 문제(TSP)와 같은 NP-hard 문제의 근사해를 구하는 데 활용됩니다.

### 그리디 전략과의 관계

최소 신장 트리 알고리즘(크루스칼, 프림)은 대표적인 그리디 알고리즘으로, 각 단계에서 가장 가중치가 작은 간선을 선택하는 지역적 최적 선택이 전체 문제의 최적해를 보장합니다.

최소 신장 트리는 그래프 이론의 기본적인 개념이자 네트워크 설계와 최적화 문제 해결에 중요한 도구입니다.

---

## 크루스칼 알고리즘(Kruskal's Algorithm)

크루스칼 알고리즘은 가중치가 있는 무방향 그래프에서 최소 신장 트리(MST)를 찾는 대표적인 알고리즘입니다. 이 알고리즘은 1956년 조셉 크루스칼(Joseph Kruskal)에 의해 개발되었습니다.

### 알고리즘 작동 원리

1. **간선 정렬**: 모든 간선을 가중치에 따라 오름차순으로 정렬합니다.
2. **간선 선택**: 가중치가 가장 작은 간선부터 차례로 선택합니다.
3. **사이클 검사**: 선택한 간선이 이미 선택된 간선들과 사이클을 형성하는지 확인합니다.
    - 사이클을 형성하지 않으면: 해당 간선을 MST에 포함시킵니다.
    - 사이클을 형성하면: 해당 간선을 무시하고 다음 간선으로 넘어갑니다.
4. **종료 조건**: (정점 수 - 1)개의 간선이 선택될 때까지 2-3단계를 반복합니다.

### 사이클 검사: Union-Find 자료구조

크루스칼 알고리즘에서는 사이클 검사를 위해 주로 Union-Find(서로소 집합, Disjoint Set) 자료구조를 사용합니다:

1. **초기화**: 각 정점이 자신만 포함하는 집합의 대표가 됩니다.
2. **Find 연산**: 해당 정점이 속한 집합의 대표 원소를 찾습니다.
3. **Union 연산**: 두 집합을 하나로 합칩니다.

### 코드 구현 예시

```python
def kruskal(graph, vertices):
    result = []  # MST를 저장할 리스트
    i, e = 0, 0  # i: 간선 인덱스, e: 결과에 포함된 간선 수
    
    # 간선을 가중치 기준으로 정렬
    graph = sorted(graph, key=lambda item: item[2])
    
    # 부모 테이블과 랭크 테이블 초기화
    parent = [i for i in range(vertices)]
    rank = [0] * vertices
    
    # 경로 압축을 사용한 Find 연산
    def find(i):
        if parent[i] != i:
            parent[i] = find(parent[i])  # 경로 압축
        return parent[i]
    
    # Union by Rank를 사용한 Union 연산
    def union(i, j):
        root_i = find(i)
        root_j = find(j)
        
        if root_i != root_j:
            if rank[root_i] < rank[root_j]:
                parent[root_i] = root_j
            else:
                parent[root_j] = root_i
                if rank[root_i] == rank[root_j]:
                    rank[root_i] += 1
    
    # MST 구성
    while e < vertices - 1:
        if i >= len(graph):
            break
            
        u, v, w = graph[i]  # u, v: 정점, w: 가중치
        i += 1
        
        x = find(u)
        y = find(v)
        
        # 사이클을 형성하지 않는 경우에만 선택
        if x != y:
            e += 1
            result.append([u, v, w])
            union(x, y)
    
    return result
```

### 시간 복잡도 분석

- **간선 정렬**: O(E log E) (E는 간선의 수)
- **Union-Find 연산**: 거의 O(1) (경로 압축과 Union by Rank 사용 시)
- **전체 알고리즘**: O(E log E) 또는 O(E log V) (E ≤ V²이므로)

### 크루스칼 알고리즘의 특징

1. **그리디 알고리즘**: 각 단계에서 가장 가중치가 작은 간선을 선택합니다.
2. **희소 그래프에 효율적**: 간선이 적은 그래프에서 프림 알고리즘보다 효율적일 수 있습니다.
3. **분리된 그래프에도 적용 가능**: 초기에 연결되지 않은 그래프에서도 MST를 구성할 수 있습니다.

크루스칼 알고리즘은 네트워크 설계, 클러스터링, 근사 알고리즘 등 다양한 분야에서 활용되는 효율적인 그래프 알고리즘입니다.

---

## 프림 알고리즘(Prim's Algorithm)

프림 알고리즘은 가중치가 있는 무방향 연결 그래프에서 최소 신장 트리(MST)를 찾는 대표적인 알고리즘입니다. 로버트 프림(Robert C. Prim)이 1957년에 개발했으며, 다익스트라 알고리즘과 유사한 접근 방식을 사용합니다.

### 알고리즘 작동 원리

1. **시작 정점 선택**: 임의의 정점을 시작점으로 선택합니다.
2. **트리 확장**: 현재 트리에 포함된 정점과 포함되지 않은 정점 사이의 간선 중 가중치가 최소인 간선을 선택합니다.
3. **정점 추가**: 선택한 간선이 연결하는 새로운 정점을 트리에 추가합니다.
4. **종료 조건**: 모든 정점이 트리에 포함될 때까지 2-3단계를 반복합니다.

### 프림 알고리즘 구현 방법

#### 우선순위 큐를 사용한 구현

```python
import heapq

def prim(graph, start_vertex):
    num_vertices = len(graph)
    # 최소 신장 트리에 포함된 간선들
    mst = []
    # 방문한 정점 추적
    visited = [False] * num_vertices
    # 간선 가중치를 저장할 우선순위 큐
    min_heap = []
    
    # 시작 정점에서 시작
    visited[start_vertex] = True
    
    # 시작 정점과 연결된 모든 간선을 우선순위 큐에 추가
    for neighbor, weight in graph[start_vertex]:
        heapq.heappush(min_heap, (weight, start_vertex, neighbor))
    
    # MST 구성
    while min_heap:
        # 가장 가중치가 작은 간선 선택
        weight, u, v = heapq.heappop(min_heap)
        
        # 이미 방문한 정점이면 스킵
        if visited[v]:
            continue
        
        # 방문 표시
        visited[v] = True
        # MST에 간선 추가
        mst.append((u, v, weight))
        
        # 새로 추가된 정점과 연결된 모든 간선 검사
        for neighbor, edge_weight in graph[v]:
            if not visited[neighbor]:
                heapq.heappush(min_heap, (edge_weight, v, neighbor))
    
    return mst
```

### 시간 복잡도 분석

- **우선순위 큐 구현**: O(E log V) (E는 간선의 수, V는 정점의 수)
- **인접 행렬 구현**: O(V²) (밀집 그래프에 효율적)

### 프림 vs 크루스칼 알고리즘

#### 프림 알고리즘

- 항상 연결된 부분 그래프를 유지하며 확장
- 밀집 그래프(간선이 많은 경우)에 효율적
- 하나의 시작점에서 출발

#### 크루스칼 알고리즘

- 간선을 개별적으로 선택하여 부분 그래프들이 최종적으로 연결됨
- 희소 그래프(간선이 적은 경우)에 효율적
- 전체 간선을 정렬하고 시작

### 프림 알고리즘의 특징

1. **그리디 알고리즘**: 각 단계에서 최소 가중치 간선을 선택합니다.
2. **항상 연결된 상태 유지**: 알고리즘 진행 중 항상 하나의 연결된 컴포넌트를 유지합니다.
3. **단일 시작점**: 한 정점에서 시작하여 트리를 확장합니다.
4. **밀집 그래프에 효율적**: 간선이 많은 그래프에서 크루스칼보다 효율적일 수 있습니다.

프림 알고리즘은 통신 네트워크, 전기 회로, 파이프라인 설계 등 다양한 분야에서 최소 비용으로 모든 노드를 연결하는 문제를 해결하는 데 사용됩니다.

---

- T(n) = T(n-1) + θ(n) → T(n) = θ(n²) (평방형의 복잡도 시간)